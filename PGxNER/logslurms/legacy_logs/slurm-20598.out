Running on sh02
# conda environments:
#
base                  *  /opt/conda
PGx_env                  /usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env
PGx_env_37               /usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env_37

pre. tgt. for `dev`:   0%|          | 0/1097 [00:00<?, ?it/s]pre. tgt. for `dev`:  11%|█         | 122/1097 [00:00<00:00, 1218.32it/s]pre. tgt. for `dev`:  27%|██▋       | 293/1097 [00:00<00:00, 1505.29it/s]pre. tgt. for `dev`:  43%|████▎     | 474/1097 [00:00<00:00, 1643.04it/s]pre. tgt. for `dev`:  60%|█████▉    | 654/1097 [00:00<00:00, 1700.95it/s]pre. tgt. for `dev`:  76%|███████▋  | 838/1097 [00:00<00:00, 1748.87it/s]pre. tgt. for `dev`:  93%|█████████▎| 1020/1097 [00:00<00:00, 1771.62it/s]                                                                          pre. tgt. for `test`:   0%|          | 0/1160 [00:00<?, ?it/s]pre. tgt. for `test`:  17%|█▋        | 200/1160 [00:00<00:00, 1994.47it/s]pre. tgt. for `test`:  34%|███▍      | 400/1160 [00:00<00:00, 1931.61it/s]pre. tgt. for `test`:  51%|█████     | 594/1160 [00:00<00:00, 1898.13it/s]pre. tgt. for `test`:  68%|██████▊   | 784/1160 [00:00<00:00, 1882.01it/s]pre. tgt. for `test`:  84%|████████▍ | 973/1160 [00:00<00:00, 1802.94it/s]                                                                          pre. tgt. for `train`:   0%|          | 0/5340 [00:00<?, ?it/s]pre. tgt. for `train`:   4%|▎         | 187/5340 [00:00<00:02, 1860.32it/s]pre. tgt. for `train`:   7%|▋         | 374/5340 [00:00<00:02, 1768.86it/s]pre. tgt. for `train`:  10%|█         | 552/5340 [00:00<00:02, 1752.11it/s]pre. tgt. for `train`:  14%|█▍        | 737/5340 [00:00<00:02, 1787.65it/s]pre. tgt. for `train`:  17%|█▋        | 927/5340 [00:00<00:02, 1817.30it/s]pre. tgt. for `train`:  21%|██        | 1118/5340 [00:00<00:02, 1844.28it/s]pre. tgt. for `train`:  24%|██▍       | 1303/5340 [00:00<00:02, 1834.76it/s]pre. tgt. for `train`:  28%|██▊       | 1487/5340 [00:00<00:02, 1824.46it/s]pre. tgt. for `train`:  31%|███▏      | 1671/5340 [00:00<00:02, 1825.67it/s]pre. tgt. for `train`:  35%|███▍      | 1854/5340 [00:01<00:01, 1787.11it/s]pre. tgt. for `train`:  39%|███▊      | 2059/5340 [00:01<00:01, 1861.49it/s]pre. tgt. for `train`:  42%|████▏     | 2263/5340 [00:01<00:01, 1906.78it/s]pre. tgt. for `train`:  46%|████▌     | 2464/5340 [00:01<00:01, 1936.05it/s]pre. tgt. for `train`:  50%|█████     | 2671/5340 [00:01<00:01, 1971.07it/s]pre. tgt. for `train`:  54%|█████▍    | 2873/5340 [00:01<00:01, 1980.81it/s]pre. tgt. for `train`:  58%|█████▊    | 3072/5340 [00:01<00:01, 1874.04it/s]pre. tgt. for `train`:  62%|██████▏   | 3289/5340 [00:01<00:01, 1956.62it/s]pre. tgt. for `train`:  65%|██████▌   | 3486/5340 [00:01<00:00, 1868.31it/s]pre. tgt. for `train`:  69%|██████▉   | 3695/5340 [00:01<00:00, 1930.86it/s]pre. tgt. for `train`:  73%|███████▎  | 3890/5340 [00:02<00:00, 1912.65it/s]pre. tgt. for `train`:  76%|███████▋  | 4083/5340 [00:02<00:00, 1911.78it/s]pre. tgt. for `train`:  80%|████████  | 4280/5340 [00:02<00:00, 1924.23it/s]pre. tgt. for `train`:  84%|████████▍ | 4481/5340 [00:02<00:00, 1947.00it/s]pre. tgt. for `train`:  88%|████████▊ | 4677/5340 [00:02<00:00, 1840.01it/s]pre. tgt. for `train`:  91%|█████████ | 4872/5340 [00:02<00:00, 1868.57it/s]pre. tgt. for `train`:  95%|█████████▌| 5085/5340 [00:02<00:00, 1932.41it/s]pre. tgt. for `train`:  99%|█████████▉| 5280/5340 [00:02<00:00, 1930.69it/s]                                                                            Save cache to caches/data_facebook/bart-large_CADEC_word.pt.
max_len_a:1.6, max_len:10
In total 3 datasets:
	dev has 1097 instances.
	test has 1160 instances.
	train has 5340 instances.

The number of tokens in tokenizer  50265
50266 50271
Traceback (most recent call last):
  File "./BARTNER/train.py", line 228, in <module>
    trainer = Trainer(train_data=ds, model=model, optimizer=optimizer,
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/trainer.py", line 559, in __init__
    _check_code(dataset=train_data, model=self.model, losser=losser, forward_func=self._forward_func, metrics=metrics,
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/trainer.py", line 959, in _check_code
    for batch_count, (batch_x, batch_y) in enumerate(_iter):
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/batch.py", line 266, in __iter__
    for indices, batch_x, batch_y in self.dataiter:
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/batch.py", line 90, in collate_fn
    sin_y = _pad(sin_y, dataset=self.dataset, as_numpy=self.as_numpy)
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/batch.py", line 43, in _pad
    res = f.pad(vlist)
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/field.py", line 492, in pad
    return self.padder(contents, field_name=self.name, field_ele_dtype=self.dtype, dim=self._cell_ndim)
  File "/usr/users/rattrapagemehdibenghali/benghali_meh/.local/lib/python3.8/site-packages/fastNLP/core/field.py", line 247, in __call__
    return np.array(contents)
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
