Running on sh09
# conda environments:
#
base                  *  /opt/conda
PGx_env                  /usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env
PGx_env_37               /usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/PGx_env_37
new_PGx_env              /usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/new_PGx_env
torch_from_source_env     /usr/users/rattrapagemehdibenghali/benghali_meh/.conda/envs/torch_from_source_env

Namespace(bart_name='facebook/bart-large', batch_size=8, dataset_name='PGxCorpus', decoder_type='avg_feature', eval_start_epoch=10, length_penalty=1, lr=1e-05, max_len=10, max_len_a=1.6, n_epochs=11, num_beams=4, save_model=1, schedule='linear', target_type='word', use_encoder_mlp=1, warmup_ratio=0.01)
Save cache to caches/data_facebook/bart-large_PGxCorpus_word.pt.
max_len_a:1.6, max_len:10
In total 3 datasets:
	dev has 75 instances.
	test has 76 instances.
	train has 567 instances.

The number of tokens in tokenizer  50265
50275 50280
cpu
11.3
input fields after batch(if batch size is 2):
	tgt_tokens: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 34]) 
	src_tokens: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 56]) 
	first: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 56]) 
	src_seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
	tgt_seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	entities: (1)type:numpy.ndarray (2)dtype:object, (3)shape:(2,) 
	tgt_tokens: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 34]) 
	target_span: (1)type:numpy.ndarray (2)dtype:object, (3)shape:(2,) 
	tgt_seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2023-02-16-13-57-52-236056
Evaluate data in 79.86 seconds!
Evaluate data in 91.97 seconds!
FitlogCallback evaluation on data-test:
Seq2SeqSpanMetric: f=37.7, rec=29.9, pre=50.970000000000006, em=0.0
Evaluation on dev at Epoch 11/11. Step:781/781: 
Seq2SeqSpanMetric: f=41.33, rec=32.66, pre=56.25, em=0.0133

Best test performance(may not correspond to the best dev performance):{'Seq2SeqSpanMetric': {'f': 37.7, 'rec': 29.9, 'pre': 50.970000000000006, 'em': 0.0}} achieved at Epoch:11.
Best test performance(correspond to the best dev performance):{'Seq2SeqSpanMetric': {'f': 37.7, 'rec': 29.9, 'pre': 50.970000000000006, 'em': 0.0}} achieved at Epoch:11.

In Epoch:11/Step:781, got best dev performance:
Seq2SeqSpanMetric: f=41.33, rec=32.66, pre=56.25, em=0.0133
